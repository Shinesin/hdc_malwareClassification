{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a627d7ce-ba93-4f21-a6e3-246e985d2e91",
   "metadata": {},
   "source": [
    "# Training, Fine-tuning, Evaluating, HDC and other models on Malware Classification Task\n",
    "## Xiangsheng Gu & Robert Armen Missirian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c15612-ffd0-4482-bec1-29764e251506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "import pyRAPL\n",
    "import random\n",
    "import warnings\n",
    "import linecache\n",
    "import tracemalloc\n",
    "import energyusage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# time measurement\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# data and models\n",
    "from SVM import SVM\n",
    "from MLP import MLP\n",
    "\n",
    "from OnlineHDv1 import OnlineHDv1\n",
    "from OnlineHDv2 import OnlineHDv2\n",
    "from NeuralHDv1 import NeuralHDv1\n",
    "from NeuralHDv2 import NeuralHDv2\n",
    "from testcode import NeuralHDSpecial\n",
    "\n",
    "from OnlineHD_model import *\n",
    "from NeuralHD_model import *\n",
    "from malware_data_preprocessing import *\n",
    "\n",
    "# Sets the seed for generating random numbers, which helps replicate runs for debugging purposes.\n",
    "# torch.manual_seed(5)\n",
    "# random.seed(5)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9c6cb-b2b2-4c03-bbe7-a63e7dfd0b05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16956c97-8f84-43a4-920a-7838ea203d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/tracemalloc.html\n",
    "def display_top(snapshot, key_type='lineno', limit=10):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, frame.filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dde6459-4545-432d-9c9c-569603e7ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data:\n",
      "x_train -> (8151, 257)\tx_test -> (2717, 257)\n",
      "y_train -> (8151,)\ty_test -> (2717,)\n",
      "This dataset contains 9 classes, and 257 features.\n",
      "Top 10 lines\n",
      "#1: C:\\Users\\WillGu\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:316: 734.3 KiB\n",
      "    f = lambda x: x.split(pat, n)\n",
      "#2: C:\\Users\\WillGu\\anaconda3\\lib\\selectors.py:315: 288.1 KiB\n",
      "    r, w, x = select.select(r, w, w, timeout)\n",
      "#3: C:\\Users\\WillGu\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1957: 85.1 KiB\n",
      "    stacked = np.empty(shape, dtype=dtype)\n",
      "#4: C:\\Users\\WillGu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:179: 85.1 KiB\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "#5: C:\\Users\\WillGu\\anaconda3\\lib\\abc.py:123: 41.7 KiB\n",
      "    return _abc_subclasscheck(cls, subclass)\n",
      "#6: <__array_function__ internals>:5: 3.9 KiB\n",
      "#7: <__array_function__ internals>:4: 3.6 KiB\n",
      "#8: C:\\Users\\WillGu\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:223: 3.5 KiB\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "#9: C:\\Users\\WillGu\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: 2.6 KiB\n",
      "    return f(*args, **kwargs)\n",
      "#10: C:\\Users\\WillGu\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:153: 1.6 KiB\n",
      "    self._date_conv = _make_date_converter(\n",
      "480 other: 199.8 KiB\n",
      "Total allocated size: 1449.3 KiB\n",
      "Memory Usage (in kilobytes): Current used -> 2139.140625 KB \t Peak used -> 70786.2099609375 KB\n",
      "1449.34375\n"
     ]
    }
   ],
   "source": [
    "tracemalloc.start()\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_dataset(\"Microsoft Challenge BIG 2015\")\n",
    "Classes = 9\n",
    "Features = 257\n",
    "print(f'Shape of input data:\\nx_train -> {x_train.shape}\\tx_test -> {x_test.shape}')\n",
    "print(f'y_train -> {y_train.shape}\\ty_test -> {y_test.shape}')\n",
    "print(f'This dataset contains {Classes} classes, and {Features} features.')\n",
    "\n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "display_top(snapshot)\n",
    "\n",
    "currentUsed  = tracemalloc.get_traced_memory()[0] / 1024\n",
    "peakUsed = tracemalloc.get_traced_memory()[1] / 1024\n",
    "print(f'Memory Usage (in kilobytes): Current used -> {currentUsed} KB \\t Peak used -> {peakUsed} KB')\n",
    "total = sum(stat.size for stat in snapshot.statistics('lineno')) / 1024\n",
    "print(total)\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058473a-9914-4051-8616-8644bfee3539",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trainning on OnlineHD, NeuralHD, MLP, SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c639b3-1e46-443d-bb3c-d45dc5e0bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataRatio_Evaluator(x_train, x_test, y_train, y_test, ratio_list, \n",
    "                        HDC = True, repetitions = 1, model_select = None,                \n",
    "                        Classes = 9, Features = 257, Dimensions = 1024,\n",
    "                        Learning_rate = 0.02, Epochs = 64, Batch_size = 64,\n",
    "                        Regeneration = 5, FractionDrop = 0.1):\n",
    "    \n",
    "    if HDC == True:\n",
    "        # For the training set\n",
    "        x_train = torch.from_numpy(x_train)\n",
    "        y_train = torch.from_numpy(y_train)\n",
    "        # For the testing set\n",
    "        x_test = torch.from_numpy(x_test)\n",
    "        y_test = torch.from_numpy(y_test)\n",
    "    \n",
    "    # initialize all evaluation factors\n",
    "    acc_means = np.zeros((len(ratio_list), ))\n",
    "    acc_medians = np.zeros((len(ratio_list), ))\n",
    "    acc_deviations = np.zeros((len(ratio_list), ))\n",
    "    \n",
    "    cpu_usage_lst = np.zeros((len(ratio_list), ))\n",
    "    memory_usage_medians = np.zeros((len(ratio_list), ))\n",
    "    training_latency_medians = np.zeros((len(ratio_list), ))\n",
    "    energy_consumption_medians = np.zeros((len(ratio_list), ))\n",
    "    \n",
    "    print(f'Training on CPU i9-12900K that has {os.cpu_count()} cores')\n",
    "    # Getting cpu load over 5 minutes\n",
    "    load1, load5, load15, = psutil.getloadavg()\n",
    "    \n",
    "    for idx, ratio in enumerate(ratio_list):\n",
    "        acc_lst = np.zeros((repetitions, ))\n",
    "        Tr_latency_lst = np.zeros((repetitions, ))\n",
    "        kiB_list = np.zeros((repetitions, ))\n",
    "        \n",
    "        data_amount = int(x_train.shape[0] * ratio)\n",
    "        \n",
    "        # CPU load for 1, 5, 15 minutes of measurement\n",
    "        load1, load5, load15, = psutil.getloadavg()\n",
    "        \n",
    "        for repetition in range(0, repetitions):\n",
    "            \n",
    "            if model_select == 'OnlineHDv1':\n",
    "                model = OnlineHDv1(classes = Classes, features = Features, \n",
    "                                   dim = Dimensions, batch_size = Batch_size,\n",
    "                                   lr = Learning_rate)\n",
    "                # if torch.cuda.is_available():\n",
    "                #     print(f'Training on {torch.cuda.get_device_name(0)}\\n')\n",
    "                #     model = model.to('cuda')\n",
    "                #     x_train = x_train.to('cuda')\n",
    "                #     y_train = y_train.to('cuda')\n",
    "                #     x_test = x_test.to('cuda')\n",
    "                #     y_test = y_test.to('cuda')\n",
    "                \n",
    "            if model_select == 'OnlineHDv2':\n",
    "                model = OnlineHDv2(classes = Classes, features = Features, \n",
    "                                   dim = Dimensions, batch_size = Batch_size,\n",
    "                                   lr = Learning_rate)\n",
    "                \n",
    "            if model_select == 'NeuralHDv1':\n",
    "                model = NeuralHDv1(classes = Classes, features = Features, \n",
    "                                   dim = Dimensions, batch_size = Batch_size, \n",
    "                                   lr = Learning_rate)\n",
    "                \n",
    "            if model_select == 'NeuralHDv2':\n",
    "                model = NeuralHDv2(classes = Classes, features = Features, \n",
    "                                   dim = Dimensions, batch_size = Batch_size, \n",
    "                                   lr = Learning_rate)  \n",
    "                \n",
    "            if model_select == 'NeuralHDSpecial':\n",
    "                model = NeuralHDSpecial(classes = Classes, features = Features, \n",
    "                                        dim = Dimensions, batch_size = Batch_size, \n",
    "                                        trainopt = 2, bestinclass = True, lr = Learning_rate)\n",
    "            \n",
    "            # start training latency\n",
    "            training_start = time.time() \n",
    "            \n",
    "            if model_select == 'OnlineHDv1' or model_select == 'OnlineHDv2': \n",
    "                # Start tracing Python memory allocations\n",
    "                tracemalloc.start()\n",
    "                model.fit(x_train[:data_amount, :], y_train[:data_amount], epochs = Epochs)\n",
    "                # measure in kilobytes\n",
    "                total_allocated_memory = sum(stat.size for stat in snapshot.statistics('lineno')) / 1024\n",
    "                # Stop tracing Python memory allocations\n",
    "                tracemalloc.stop()\n",
    "\n",
    "            if model_select == 'NeuralHDv1' or model_select == 'NeuralHDv2' or model_select == 'NeuralHDSpecial': \n",
    "                # Start tracing Python memory allocations\n",
    "                tracemalloc.start()\n",
    "                model.fit(x_train[:data_amount, :], y_train[:data_amount], \n",
    "                          epochs = Epochs, regenloops = Regeneration, fractionToDrop = FractionDrop)\n",
    "                # measure in kilobytes\n",
    "                total_allocated_memory = sum(stat.size for stat in snapshot.statistics('lineno')) / 1024\n",
    "                # Stop tracing Python memory allocations\n",
    "                tracemalloc.stop()\n",
    "            \n",
    "            # complete training\n",
    "            training_end = time.time()\n",
    "            training_latency_sec = training_end - training_start\n",
    "            \n",
    "            y_pred = model(x_test)\n",
    "            binary_eval = [y_pred[i] == y_test[i] for i in range(len(y_test))]\n",
    "            accuracy = sum(binary_eval) / len(y_test)\n",
    "            \n",
    "            acc_lst[repetition] = accuracy \n",
    "            Tr_latency_lst[repetition] = training_latency_sec\n",
    "            kiB_list[repetition] = total_allocated_memory\n",
    "\n",
    "            \n",
    "        avg = np.mean(acc_lst)\n",
    "        median = np.median(acc_lst)\n",
    "        std = np.std(acc_lst) \n",
    "        Tr_latency_median = np.median(Tr_latency_lst)\n",
    "        kiB_median = np.median(kiB_list)\n",
    "        \n",
    "        acc_means[idx] = avg\n",
    "        acc_medians[idx] = median\n",
    "        acc_deviations[idx] = std\n",
    "        training_latency_medians[idx] = Tr_latency_median\n",
    "        memory_usage_medians[idx] = kiB_median\n",
    "        \n",
    "        # Getting cpu load over 5 minutes for each ratio\n",
    "        cpu_usage = (load5 / os.cpu_count()) * 100\n",
    "        cpu_usage_lst[idx] = cpu_usage\n",
    "    \n",
    "    print(f'{model_select} Report:')\n",
    "    print(f'Classes -> {Classes} \\t Features -> {Features} \\t Dimensions -> {Dimensions}')\n",
    "    print(f'Learning Rate -> {Learning_rate} \\t Epochs -> {Epochs} \\t Batch size -> {Batch_size}\\n')\n",
    "    if model_select == 'NeuralHDv1' or model_select == 'NeuralHDv2' or model_select == 'NeuralHDSpecial':\n",
    "        print(f'Regeneration -> {Regeneration} \\t FractionDrop -> {FractionDrop}\\n')\n",
    "        \n",
    "    print(f'Data Usage Ratio: {ratio_list}\\nAverage Accuracy: {acc_means}\\nMedian Accuracy: {acc_medians}')\n",
    "    print(f'Accuracy Deviation: {acc_deviations}\\nTraining Latency: {training_latency_medians}')\n",
    "    print(f'CPU Usage(%): {cpu_usage_lst}\\nMemory Usage(KiB): {memory_usage_medians}')\n",
    "          \n",
    "    return [ratio_list, acc_means, acc_medians, acc_deviations, training_latency_medians, cpu_usage_lst, memory_usage_medians]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee4e992-b10c-4927-aeba-b4eb8e904cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU i9-12900K that has 24 cores\n",
      "OnlineHDv1 Report:\n",
      "Classes -> 9 \t Features -> 257 \t Dimensions -> 2048\n",
      "Learning Rate -> 0.00035 \t Epochs -> 64 \t Batch size -> 64\n",
      "\n",
      "Data Usage Ratio: [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
      "Average Accuracy: [0.84545454 0.87489878 0.8993375  0.90647774 0.92892896 0.93577475\n",
      " 0.93713655 0.94497607 0.94622746 0.9507545  0.94799411]\n",
      "Median Accuracy: [0.85259476 0.87780643 0.90504232 0.90725064 0.92914978 0.9381671\n",
      " 0.93632683 0.94608024 0.94608024 0.95086491 0.9479205 ]\n",
      "Accuracy Deviation: [0.02120857 0.02298752 0.01731902 0.01412139 0.00561917 0.00556297\n",
      " 0.00451912 0.0033622  0.00427781 0.00420226 0.00489953]\n",
      "Training Latency: [0.44624639 0.90399241 1.65898621 2.42673004 3.12897468 3.85997152\n",
      " 4.62047029 5.34921563 6.07846069 6.81220567 7.61472166]\n",
      "CPU Usage(%): [ 0.          0.          2.08333333  4.625       4.58333333  7.08333333\n",
      "  8.125      11.83333333 10.16666667 11.875      12.95833333]\n",
      "Memory Usage(KiB): [1449.34375 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375\n",
      " 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375]\n"
     ]
    }
   ],
   "source": [
    "Ratio_list = [.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "\n",
    "Results = DataRatio_Evaluator(x_train, x_test, y_train, y_test, ratio_list = Ratio_list, \n",
    "                              HDC = True, repetitions = 10, model_select = 'OnlineHDv1',                \n",
    "                              Classes = 9, Features = 257, Dimensions = 2048,\n",
    "                              Learning_rate = .00035, Epochs = 64, Batch_size = 64,\n",
    "                              Regeneration = 5, FractionDrop = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c5fce0-a307-43b1-a9f1-861d068861aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU i9-12900K that has 24 cores\n",
      "OnlineHDv2 Report:\n",
      "Classes -> 9 \t Features -> 257 \t Dimensions -> 2048\n",
      "Learning Rate -> 0.00035 \t Epochs -> 64 \t Batch size -> 64\n",
      "\n",
      "Data Usage Ratio: [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
      "Average Accuracy: [0.83143173 0.80033125 0.91310269 0.91733531 0.91689364 0.93444976\n",
      " 0.93341921 0.93584836 0.9367317  0.93702614 0.9423629 ]\n",
      "Median Accuracy: [0.8522267  0.87154952 0.91847628 0.9241811  0.92730954 0.93595877\n",
      " 0.9431358  0.93853515 0.94350389 0.94000739 0.94552815]\n",
      "Accuracy Deviation: [0.05968566 0.12162392 0.01945567 0.01657073 0.0232479  0.01341901\n",
      " 0.01958821 0.01448656 0.02006847 0.01598422 0.00944857]\n",
      "Training Latency: [0.48274684 0.92849386 1.70148921 2.46473432 3.08673036 3.85347462\n",
      " 4.62046993 5.29496574 5.96221197 6.69045758 7.21920347]\n",
      "CPU Usage(%): [13.83333333 14.         13.54166667 12.875      12.91666667 12.25\n",
      " 10.70833333 14.25       12.91666667 13.125      11.41666667]\n",
      "Memory Usage(KiB): [1449.34375 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375\n",
      " 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375]\n"
     ]
    }
   ],
   "source": [
    "Results = DataRatio_Evaluator(x_train, x_test, y_train, y_test, ratio_list = Ratio_list, \n",
    "                              HDC = True, repetitions = 10, model_select = 'OnlineHDv2',                \n",
    "                              Classes = 9, Features = 257, Dimensions = 2048,\n",
    "                              Learning_rate = .00035, Epochs = 64, Batch_size = 64,\n",
    "                              Regeneration = 5, FractionDrop = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5e1257-c916-4597-bc20-58d7f5f47504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU i9-12900K that has 24 cores\n",
      "NeuralHDv1 Report:\n",
      "Classes -> 9 \t Features -> 257 \t Dimensions -> 2048\n",
      "Learning Rate -> 0.001 \t Epochs -> 5 \t Batch size -> 32\n",
      "\n",
      "Regeneration -> 25 \t FractionDrop -> 0.4\n",
      "\n",
      "Data Usage Ratio: [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
      "Average Accuracy: [0.81924917 0.81829224 0.76190652 0.89955835 0.91479573 0.91104159\n",
      " 0.90419581 0.92469636 0.89988958 0.93191019 0.91972764]\n",
      "Median Accuracy: [0.81818181 0.85756347 0.77585572 0.90209791 0.91700405 0.91221938\n",
      " 0.9146117  0.92510122 0.90136179 0.93687889 0.91994846]\n",
      "Accuracy Deviation: [0.02230076 0.08700262 0.11133544 0.01602654 0.00734124 0.01632683\n",
      " 0.02624972 0.00576845 0.01832815 0.01382821 0.01771895]\n",
      "Training Latency: [ 1.83523858  3.33472872  6.1296562   9.01996279 11.71784198 14.66868901\n",
      " 17.43528354 20.30304301 23.15546739 25.93422616 28.81345642]\n",
      "CPU Usage(%): [10.95833333 12.75       19.04166667 18.79166667 15.125      10.625\n",
      "  9.08333333  5.91666667  4.79166667  7.29166667  4.41666667]\n",
      "Memory Usage(KiB): [1449.34375 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375\n",
      " 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375]\n"
     ]
    }
   ],
   "source": [
    "Results = DataRatio_Evaluator(x_train, x_test, y_train, y_test, ratio_list = Ratio_list, \n",
    "                              HDC = True, repetitions = 10, model_select = 'NeuralHDv1',                \n",
    "                              Classes = 9, Features = 257, Dimensions = 2048,\n",
    "                              Learning_rate = .001, Epochs = 5, Batch_size = 32,\n",
    "                              Regeneration = 25, FractionDrop = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9057039a-bf8b-48f5-aa9e-94d45bb42a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU i9-12900K that has 24 cores\n",
      "NeuralHDv2 Report:\n",
      "Classes -> 9 \t Features -> 257 \t Dimensions -> 2048\n",
      "Learning Rate -> 0.001 \t Epochs -> 5 \t Batch size -> 32\n",
      "\n",
      "Regeneration -> 25 \t FractionDrop -> 0.4\n",
      "\n",
      "Data Usage Ratio: [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
      "Average Accuracy: [0.83687891 0.85160104 0.84556496 0.87338977 0.87935223 0.91420685\n",
      " 0.91177769 0.90846522 0.91615751 0.90835481 0.8889216 ]\n",
      "Median Accuracy: [0.84946632 0.87062937 0.86897314 0.89565697 0.88479942 0.91626796\n",
      " 0.91479573 0.91405964 0.92749354 0.90982702 0.92988589]\n",
      "Accuracy Deviation: [0.0392555  0.07266616 0.08086489 0.05455608 0.04540253 0.02107274\n",
      " 0.01998094 0.03099699 0.0241381  0.01871216 0.08387262]\n",
      "Training Latency: [ 1.4048568   2.56269193  4.53706026  6.35863924  8.24099267 10.41164136\n",
      " 12.50537539 14.56478238 16.63876629 18.59641671 20.5607549 ]\n",
      "CPU Usage(%): [4.25       4.16666667 3.83333333 3.5        3.29166667 4.33333333\n",
      " 4.45833333 4.375      5.08333333 4.125      5.04166667]\n",
      "Memory Usage(KiB): [1449.34375 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375\n",
      " 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375]\n"
     ]
    }
   ],
   "source": [
    "Results = DataRatio_Evaluator(x_train, x_test, y_train, y_test, ratio_list = Ratio_list, \n",
    "                              HDC = True, repetitions = 10, model_select = 'NeuralHDv2',                \n",
    "                              Classes = 9, Features = 257, Dimensions = 2048,\n",
    "                              Learning_rate = .001, Epochs = 5, Batch_size = 32,\n",
    "                              Regeneration = 25, FractionDrop = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e6a732-d890-41d9-b7b1-29f305dad392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU i9-12900K that has 24 cores\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "NeuralHDSpecial Report:\n",
      "Classes -> 9 \t Features -> 257 \t Dimensions -> 2048\n",
      "Learning Rate -> 0.001 \t Epochs -> 5 \t Batch size -> 32\n",
      "\n",
      "Regeneration -> 25 \t FractionDrop -> 0.4\n",
      "\n",
      "Data Usage Ratio: [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
      "Average Accuracy: [0.86002945 0.90047848 0.90927493 0.91972764 0.92281929 0.92465954\n",
      " 0.93113729 0.93433935 0.92929702 0.93430254 0.93305116]\n",
      "Median Accuracy: [0.85995585 0.90338609 0.91203535 0.92050055 0.92252484 0.92583731\n",
      " 0.93172616 0.93393448 0.93043798 0.93448657 0.93209422]\n",
      "Accuracy Deviation: [0.01078094 0.00629187 0.00977422 0.00703251 0.00543437 0.00925716\n",
      " 0.00691989 0.00677897 0.0087042  0.00604605 0.00802397]\n",
      "Training Latency: [ 1.54121256  2.78649986  4.94943345  7.06388593  8.86207891 10.96139014\n",
      " 13.02483439 14.80886042 17.04534674 19.12482595 21.07556319]\n",
      "CPU Usage(%): [5.08333333 4.83333333 7.875      7.375      6.5        5.33333333\n",
      " 4.58333333 6.04166667 5.08333333 6.         4.75      ]\n",
      "Memory Usage(KiB): [1449.34375 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375\n",
      " 1449.34375 1449.34375 1449.34375 1449.34375 1449.34375]\n"
     ]
    }
   ],
   "source": [
    "Results = DataRatio_Evaluator(x_train, x_test, y_train, y_test, ratio_list = Ratio_list, \n",
    "                              HDC = True, repetitions = 10, model_select = 'NeuralHDSpecial',                \n",
    "                              Classes = 9, Features = 257, Dimensions = 2048,\n",
    "                              Learning_rate = .001, Epochs = 5, Batch_size = 32,\n",
    "                              Regeneration = 25, FractionDrop = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7a40a-5cb3-4916-ac77-8e1b9bb458c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryAccuracy(y, ypred, train_or_test = 'training', report = False):\n",
    "    if type(y) == np.ndarray and type(ypred) == np.ndarray:\n",
    "        y = torch.from_numpy(y)\n",
    "        ypred = torch.from_numpy(ypred)\n",
    "        \n",
    "    lst = torch.eq(y, ypred)\n",
    "    count = 0\n",
    "    for i in lst:\n",
    "        if i == True:\n",
    "            count += 1\n",
    "    accuracy = count/len(lst) * 100\n",
    "    \n",
    "    if report == True:\n",
    "        print(f'The {train_or_test} accuracy is about: {round(accuracy, 2)}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e208efa0-a085-41f7-a840-590d7224aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                    repeat_times = 1, model_select = None, verbose = True,\n",
    "                    Classes = 9, Features = 257, Dimensions = 1024,\n",
    "                    Learning_rate = 0.02, Epochs = 64, Batch_size = 64,\n",
    "                    Regeneration = 5, FractionDrop = 0.1):\n",
    "    \n",
    "    if (model_select == 'OnlineHD') or (model_select == 'NeuralHD'):\n",
    "        # For the training set\n",
    "        x_train = torch.from_numpy(x_train)\n",
    "        y_train = torch.from_numpy(y_train)\n",
    "        # For the testing set\n",
    "        x_test = torch.from_numpy(x_test)\n",
    "        y_test = torch.from_numpy(y_test)\n",
    "        \n",
    "    # Lists for storing accuracy scores(%) and time measurements(in seconds)\n",
    "    Tr_lst = np.zeros((repeat_times, ))\n",
    "    Te_lst = np.zeros((repeat_times, ))\n",
    "    \n",
    "    Tr_latency_lst = np.zeros((repeat_times, ))\n",
    "    Inference_time_lst = np.zeros((repeat_times, ))\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('Training on 12th Gen Intel Core i9-12900K')\n",
    "    \n",
    "    for each_time in tqdm(range(repeat_times)): \n",
    "        # here is why time on loading display is larger than real runtime \n",
    "        time.sleep(2)\n",
    "        # set up training latency and inference time\n",
    "        training_start = 0.0\n",
    "        training_end = 0.0\n",
    "        infer_start = 0.0\n",
    "        infer_end = 0.0\n",
    "        \n",
    "        \n",
    "        if model_select == 'NeuralHD':\n",
    "            model = NeuralHD(classes = Classes, \n",
    "                             features = Features, \n",
    "                             dim = Dimensions, \n",
    "                             batch_size = Batch_size,\n",
    "                             trainopt = 3,\n",
    "                             bestinclass = True)\n",
    "            # start training latency\n",
    "            training_start = time.time()\n",
    "            \n",
    "            model.fit(x_train, x_test, y_train, y_test, \n",
    "                      epochs = Epochs, \n",
    "                      regenloops = Regeneration, \n",
    "                      fractionToDrop = FractionDrop)\n",
    "            \n",
    "            # complete training\n",
    "            training_end = time.time()\n",
    "        \n",
    "        \n",
    "        if model_select == 'OnlineHD':\n",
    "            model = OnlineHD(classes = Classes, \n",
    "                             features = Features, \n",
    "                             dim = Dimensions)\n",
    "\n",
    "            # Optional: For both training and testing sets, check if gpu/cuda is available to use\n",
    "            '''\n",
    "            if torch.cuda.is_available() and model_select == 'OnlineHD':\n",
    "                print(f'Training on {torch.cuda.get_device_name(0)}\\n')\n",
    "                model = model.to('cuda')\n",
    "                x_train = x_train.to('cuda')\n",
    "                y_train = y_train.to('cuda')\n",
    "                x_test = x_test.to('cuda')\n",
    "                y_test = y_test.to('cuda')\n",
    "            '''\n",
    "            \n",
    "            # start training latency\n",
    "            training_start = time.time()\n",
    "            \n",
    "            model.fit(x_train, y_train, \n",
    "                      encoded = False, \n",
    "                      lr = Learning_rate, \n",
    "                      epochs = Epochs, \n",
    "                      batch_size = Batch_size, \n",
    "                      one_pass_fit = False)\n",
    "            \n",
    "            # complete training\n",
    "            training_end = time.time()\n",
    "            \n",
    "            \n",
    "        if model_select == 'MLP':\n",
    "            model = MLP(classes = Classes,\n",
    "                        features = Features,\n",
    "                        dim = Dimensions)\n",
    "            # start training latency\n",
    "            training_start = time.time()\n",
    "            \n",
    "            model.fit(x_train, y_train,  \n",
    "                      epochs = Epochs,\n",
    "                      batch_size = Batch_size,\n",
    "                      lr = Learning_rate)\n",
    "            \n",
    "            \n",
    "            # complete training\n",
    "            training_end = time.time()\n",
    "        \n",
    "        \n",
    "        if model_select == 'SVM':\n",
    "            model = SVM()\n",
    "            \n",
    "            # start training latency\n",
    "            training_start = time.time()\n",
    "            \n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            \n",
    "            # complete training\n",
    "            training_end = time.time()        \n",
    "        \n",
    "        training_latency_sec = training_end - training_start\n",
    "        Tr_latency_lst[each_time] = training_latency_sec\n",
    "        \n",
    "        # start inference time\n",
    "        infer_start = time.time()  \n",
    "        \n",
    "        # prediction\n",
    "        ypred = model(x_train)\n",
    "        \n",
    "        # prediction completed\n",
    "        infer_end = time.time()\n",
    "        inference_time_sec = infer_end - infer_start\n",
    "        Inference_time_lst[each_time] = inference_time_sec\n",
    "        \n",
    "        Tr_accuracy = binaryAccuracy(y_train, ypred, \n",
    "                                     train_or_test = 'training', \n",
    "                                     report = False)\n",
    "        Tr_lst[each_time] = Tr_accuracy\n",
    "        \n",
    "        # prediction\n",
    "        ypred = model(x_test)\n",
    "        Te_accuracy = binaryAccuracy(y_test, ypred, \n",
    "                                     train_or_test = 'testing', \n",
    "                                     report = False)\n",
    "        Te_lst[each_time] = Te_accuracy\n",
    "        \n",
    "    # For the training set\n",
    "    # Tr_max = max(Tr_lst)\n",
    "    Tr_mean = np.mean(Tr_lst)\n",
    "    Tr_variance = np.var(Tr_lst)\n",
    "    \n",
    "    # For the testing set\n",
    "    # Te_max = max(Te_lst)\n",
    "    Te_mean = np.mean(Te_lst)\n",
    "    Te_variance = np.var(Te_lst)\n",
    "    \n",
    "    # For training latency and inference time\n",
    "    avg_training_latency = np.mean(Tr_latency_lst)\n",
    "    avg_inference_time = np.mean(Inference_time_lst)\n",
    "    \n",
    "    # The report\n",
    "    if verbose == True:\n",
    "        print(f'The applied model is {model_select}, and its hyper-parameters are set as:\\n')\n",
    "        print(f'Classes -> {Classes} \\t Features -> {Features} \\t Dimensions -> {Dimensions}')\n",
    "        print(f'Learning Rate -> {Learning_rate} \\t Epochs -> {Epochs} \\t Batch size -> {Batch_size}\\n')\n",
    "    \n",
    "    if (verbose == True) and (model_select == 'NeuralHD'):\n",
    "        print(f'Regeneration -> {Regeneration} \\t FractionDrop -> {FractionDrop}\\n')\n",
    "        \n",
    "    if (repeat_times > 1) and (verbose == True):\n",
    "        print(f'***** For {repeat_times} times of iteration *****\\n') \n",
    "        print(f'The variance on the training set: {round(Tr_variance, 4)}\\nThe variance on the testing set: {round(Te_variance, 4)}')   \n",
    "    \n",
    "    if verbose == True:        \n",
    "        print(f'The average accuracy on the training set: {round(Tr_mean, 4)}\\nThe average accuracy on the testing set: {round(Te_mean, 4)}')\n",
    "        print(f'The average training latency: {round(avg_training_latency, 4)} seconds\\nThe average inference time: {round(avg_inference_time, 4)} seconds\\n')\n",
    "    return [Tr_mean, Tr_variance, Te_mean, Te_variance, avg_training_latency, avg_inference_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b666872-93ab-4806-a7d3-ac89374b5eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning all hyperparamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521adad6-1535-4a3f-9b0a-4d88f087e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimentionality: [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176]\n",
      "Learning rate: [1e-06, 2e-06, 3e-06, 4e-06, 5e-06, 1e-05, 2e-05, 3e-05, 4e-05, 5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
      "Regenerate loops: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "Dropping rates: [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "Epochs: [25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "# Lists for iterative fine-tuning\n",
    "dim_list = [1024, 1224, 1488, 1888, 2048, 2222, 2488, 2888]\n",
    "\n",
    "print(f'Dimentionality: {dim_list}')\n",
    "\n",
    "lr_list = [0.000001, 0.000002, 0.000003, 0.000004, 0.000005, \n",
    "           0.00001, 0.00002, 0.00003, 0.00004, 0.00005,\n",
    "           0.0001, 0.0002, 0.0003, 0.0004, 0.0005,\n",
    "           0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "print(f'Learning rate: {lr_list}')\n",
    "\n",
    "regenloop = [number for number in range(11,26)]\n",
    "print(f'Regenerate loops: {regenloop}')\n",
    "\n",
    "fractiontoDrop = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                  0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7] \n",
    "print(f'Dropping rates: {fractiontoDrop}')\n",
    "\n",
    "ep_list = [number**2 for number in range(5,11)]\n",
    "print(f'Epochs: {ep_list}')\n",
    "Repeat_times = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec609e1-0f55-4597-8d69-1e448fc80ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation_Visualizer(X, Y_lst, hyperParam_name, labels = 'time'):\n",
    "    Y_labels = list()\n",
    "    print('\\nThe Analysis')\n",
    "    if labels == 'acc':\n",
    "        Y_labels = ['Training Accuracy', 'Testing Accuracy', \n",
    "                    'Training Variance', 'Testing Variance']\n",
    "        Tr_lst = Y_lst[0]\n",
    "        Te_lst = Y_lst[1]\n",
    "        Tr_var = Y_lst[2]\n",
    "        Te_var = Y_lst[3]\n",
    "        print(f'List of training accuracy: {Tr_lst}\\n List of training variance: {Tr_var}\\n List of testing variance: {Te_var}\\n List of testing accuracy{Te_lst}\\n')\n",
    "        fig, axs = plt.subplots(2, 1, figsize = (11, 15))\n",
    "        axs = np.atleast_2d(axs)\n",
    "        axs[0, 0].set_title(f'Accuracy in {hyperParam_name}')\n",
    "        axs[0, 0].plot(X, Tr_lst, lw = 2, color='blue', alpha=0.95, label = Y_labels[0])\n",
    "        axs[0, 0].plot(X, Te_lst, lw = 2, color='red', alpha=0.95, label = Y_labels[1])\n",
    "        xtr_max = X[np.argmax(Tr_lst)]\n",
    "        ytr_max = Tr_lst.max()\n",
    "        axs[0, 0].plot(xtr_max, ytr_max, marker=\"o\", markersize=7, markeredgecolor='green', markerfacecolor='blue')\n",
    "        xte_max = X[np.argmax(Te_lst)]\n",
    "        yte_max = Te_lst.max()\n",
    "        axs[0, 0].plot(xte_max, yte_max, marker=\"o\", markersize=7, markeredgecolor='green', markerfacecolor='red')\n",
    "        axs[0, 0].set(xlabel = hyperParam_name, ylabel = 'Accuracy(%)')\n",
    "        axs[0, 0].legend(fontsize = 9, loc = 1)\n",
    "        print(f'{hyperParam_name} at {xtr_max} got maximum training accuracy -> {ytr_max}')\n",
    "        print(f'{hyperParam_name} at {xte_max} got maximum testing accuracy -> {yte_max}')\n",
    "        \n",
    "        axs[0, 1].set_title(f'Variance in {hyperParam_name}')\n",
    "        axs[0, 1].plot(X, Tr_var, lw = 2, color='blue', alpha=0.95, label = Y_labels[2])\n",
    "        axs[0, 1].plot(X, Te_var, lw = 2, color='red', alpha=0.95, label = Y_labels[3])\n",
    "        xtr_min = X[np.argmin(Tr_var)]\n",
    "        ytr_min = Tr_var.min()\n",
    "        axs[0, 1].plot(xtr_min, ytr_min, marker=\"o\", markersize=7, markeredgecolor='green', markerfacecolor='blue')\n",
    "        xte_min = X[np.argmin(Te_var)]\n",
    "        yte_min = Te_var.min()\n",
    "        axs[0, 1].plot(xte_min, yte_min, marker=\"o\", markersize=7, markeredgecolor='green', markerfacecolor='red')\n",
    "        axs[0, 1].set(xlabel = hyperParam_name, ylabel = 'Variance')\n",
    "        axs[0, 1].legend(fontsize = 9, loc = 1)     \n",
    "        print(f'{hyperParam_name} at {xtr_min} got minimum training variance -> {ytr_min}')\n",
    "        print(f'{hyperParam_name} at {xte_min} got minimum testing variance -> {yte_min}')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        Y_labels = ['Training Latency', 'Inference Time']\n",
    "        Tr_latency_lst = Y_lst[0]\n",
    "        Inference_time_lst = Y_lst[1] \n",
    "        print(f'List of Training Latency: {Tr_latency_lst}\\n List of Inference Time: {Inference_time_lst}\\n')\n",
    "        fig, axs = plt.subplots(2, 1, figsize = (11, 15))\n",
    "        axs = np.atleast_2d(axs)        \n",
    "        axs[0, 0].set_title(f'Training Latency in {hyperParam_name}')\n",
    "        axs[0, 0].plot(X, Tr_latency_lst, lw = 2, color='blue', alpha=0.95, label = 'Training Latency')\n",
    "        axs[0, 0].set(xlabel = hyperParam_name, ylabel = 'Time(seconds)')\n",
    "        axs[0, 0].legend(fontsize = 9, loc = 1)\n",
    "        \n",
    "        axs[0, 1].set_title(f'Inference Time in {hyperParam_name}') \n",
    "        axs[0, 1].plot(X, Inference_time_lst, lw = 2, color='red', alpha=0.95, label = 'Inference Time')\n",
    "        axs[0, 1].set(xlabel = hyperParam_name, ylabel = 'Time(seconds)')\n",
    "        axs[0, 1].legend(fontsize = 9, loc = 1)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return 'Done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3fa1b-3dd3-4a48-a4d4-5f33f5fc72d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OnlineHD model fine-tuning\n",
    "Keep the batch size at **64**, varying the dimentionality, learning rate, and finally the epochs.\n",
    "    \n",
    "    dim_list = [512, 1024, 2048, 4096, \n",
    "                8192, 16384, 32768, 65536] -> 16384\n",
    "    \n",
    "    lr_list = [0.01, 0.02, 0.03, 0.04, 0.05,\n",
    "               0.001, 0.002, 0.003, 0.004, 0.005, \n",
    "               0.0001, 0.0002, 0.0003, 0.0004, 0.0005, \n",
    "               0.00001, 0.00002, 0.00003, 0.00004, 0.00005] -> 0.00001\n",
    "               \n",
    "    ep_list = [4, 9, 16, 25, 36, \n",
    "               49, 64, 81, 100, 121] -> 64\n",
    "               \n",
    "    Training on 12th Gen Intel Core i9-12900K\n",
    "    100%|██████████| 20/20 [05:11<00:00, 15.58s/it]\n",
    "    The applied model is OnlineHD, and its hyper-parameters are set as:\n",
    "\n",
    "    Classes -> 9 \t Features -> 257 \t Dimensions -> 16384\n",
    "    Learning Rate -> 1e-05 \t Epochs -> 64 \t Batch size -> 64\n",
    "\n",
    "    ***** For 20 times of iteration *****\n",
    "\n",
    "    The variance on the training set: 0.3912\n",
    "    The variance on the testing set: 0.4178\n",
    "    The average accuracy on the training set: 96.6176\n",
    "    The average accuracy on the testing set: 95.0221\n",
    "    The average training latency: 13.1704 seconds\n",
    "    The average inference time: 0.2481 seconds       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5b9f8f-2c94-4f98-9639-d92b621cf94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:25<00:00,  7.27s/it]\n",
      "100%|██████████| 20/20 [02:28<00:00,  7.44s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.25s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.34s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.32s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:20<00:00,  7.02s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.07s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.07s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.05s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.07s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.17s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.11s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.05s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.11s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.14s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.15s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.10s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.11s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.10s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.15s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.15s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.14s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.09s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.14s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.14s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.08s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.15s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.10s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.26s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.11s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.17s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.27s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.14s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.25s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.14s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.37s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.39s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.37s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.30s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.37s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.38s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.25s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.28s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:21<00:00,  7.06s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.11s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.26s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.17s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.17s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.27s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.16s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.29s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.26s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.25s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.27s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.37s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.30s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.18s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.15s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.30s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.32s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.28s/it]\n",
      "100%|██████████| 20/20 [02:22<00:00,  7.12s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.22s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.28s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.30s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.21s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.30s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.24s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.34s/it]\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.33s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.19s/it]\n",
      "100%|██████████| 20/20 [02:23<00:00,  7.20s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.29s/it]\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.46s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.47s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:28<00:00,  7.44s/it]\n",
      "100%|██████████| 20/20 [02:28<00:00,  7.45s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.47s/it]\n",
      "100%|██████████| 20/20 [02:28<00:00,  7.45s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.47s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.48s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.48s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.47s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.48s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.47s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.48s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:25<00:00,  7.25s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.59s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.47s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.49s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.54s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.60s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.59s/it]\n",
      "100%|██████████| 20/20 [02:34<00:00,  7.72s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.61s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.60s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.61s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:29<00:00,  7.48s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.60s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.59s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.59s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.63s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.59s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.60s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.60s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.63s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:33<00:00,  7.68s/it]\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.38s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:33<00:00,  7.65s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.61s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.62s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.64s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.60s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.63s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.61s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.62s/it]\n",
      "100%|██████████| 20/20 [02:33<00:00,  7.67s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.63s/it]\n",
      "100%|██████████| 20/20 [02:33<00:00,  7.70s/it]\n",
      "100%|██████████| 20/20 [02:33<00:00,  7.69s/it]\n",
      "100%|██████████| 20/20 [02:36<00:00,  7.84s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.58s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.57s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.55s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.51s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.59s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:31<00:00,  7.56s/it]\n",
      "100%|██████████| 20/20 [02:28<00:00,  7.45s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.53s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.50s/it]\n",
      "100%|██████████| 20/20 [02:36<00:00,  7.85s/it]\n",
      "100%|██████████| 20/20 [02:37<00:00,  7.88s/it]\n",
      "100%|██████████| 20/20 [02:37<00:00,  7.86s/it]\n",
      "100%|██████████| 20/20 [02:36<00:00,  7.83s/it]\n",
      "100%|██████████| 20/20 [02:38<00:00,  7.93s/it]\n",
      "100%|██████████| 20/20 [02:39<00:00,  8.00s/it]\n",
      "100%|██████████| 20/20 [02:39<00:00,  7.96s/it]\n",
      "100%|██████████| 20/20 [02:34<00:00,  7.72s/it]\n",
      "100%|██████████| 20/20 [02:30<00:00,  7.52s/it]\n",
      "100%|██████████| 20/20 [02:32<00:00,  7.63s/it]\n",
      " 10%|█         | 2/20 [00:21<03:17, 10.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34012/2385594695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mDimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdim_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     results = HDC_Model_Analyzer(x_train, x_test, y_train, y_test, \n\u001b[0m\u001b[0;32m     20\u001b[0m                                  \u001b[0mrepeat_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRepeat_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_select\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mThe_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                  \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34012/2478424569.py\u001b[0m in \u001b[0;36mHDC_Model_Analyzer\u001b[1;34m(x_train, x_test, y_train, y_test, repeat_times, model_select, verbose, Classes, Features, Dimensions, Learning_rate, Epochs, Batch_size, Regeneration, FractionDrop)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mtraining_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             model.fit(x_train, y_train, \n\u001b[0m\u001b[0;32m     86\u001b[0m                       \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                       \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\BIASLab_research_project\\MalwareClassification\\OnlineHD_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, encoded, lr, epochs, batch_size, one_pass_fit, bootstrap)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mone_pass_fit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_one_pass_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterative_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\BIASLab_research_project\\MalwareClassification\\OnlineHD_model.py\u001b[0m in \u001b[0;36m_iterative_fit\u001b[1;34m(self, h, y, lr, epochs, batch_size)\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[0mh_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                 \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[0mwrong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\BIASLab_research_project\\MalwareClassification\\OnlineHD_model.py\u001b[0m in \u001b[0;36mscores\u001b[1;34m(self, x, encoded)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcos_cdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\BIASLab_research_project\\MalwareClassification\\OnlineHD_model.py\u001b[0m in \u001b[0;36mcos_cdist\u001b[1;34m(x1, x2, eps)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mnorms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnorms2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mcdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mcdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setup OnlineHD model's hyper-parameters\n",
    "The_model = 'OnlineHD'\n",
    "Batch_size = 64\n",
    "# Dimension = 1024\n",
    "Learning_rate = 1e-05\n",
    "Epochs = 64\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(dim_list), ))\n",
    "Tr_variance_lst = np.zeros((len(dim_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(dim_list), ))\n",
    "Te_variance_lst = np.zeros((len(dim_list), ))\n",
    "avg_Latency_lst = np.zeros((len(dim_list), ))\n",
    "avg_Inference_lst = np.zeros((len(dim_list), ))\n",
    "\n",
    "\n",
    "for idx in range(0, len(dim_list)):\n",
    "    Dimension = dim_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(dim_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Dimension', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(dim_list, [avg_Latency_lst, avg_Inference_lst], 'Dimension', labels = 'time'))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb682805-fc73-4939-8a6b-20159db58ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup OnlineHD model's hyper-parameters\n",
    "The_model = 'OnlineHD'\n",
    "Batch_size = 64\n",
    "Dimension = 16384\n",
    "# Learning_rate = 0.003\n",
    "Epochs = 100\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(lr_list), ))\n",
    "Tr_variance_lst = np.zeros((len(lr_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(lr_list), ))\n",
    "Te_variance_lst = np.zeros((len(lr_list), ))\n",
    "avg_Latency_lst = np.zeros((len(lr_list), ))\n",
    "avg_Inference_lst = np.zeros((len(lr_list), ))\n",
    "\n",
    "for idx in range(0, len(lr_list)):\n",
    "    Learning_rate = lr_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(lr_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Learning rate', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(lr_list, [avg_Latency_lst, avg_Inference_lst], 'Learning rate', labels = 'time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ca64d-b54d-4984-9241-0973507a41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup OnlineHD model's hyper-parameters\n",
    "The_model = 'OnlineHD'\n",
    "Batch_size = 64\n",
    "Dimensions= 16384 \n",
    "Learning_rate = 1e-05\n",
    "# Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(ep_list), ))\n",
    "Tr_variance_lst = np.zeros((len(ep_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(ep_list), ))\n",
    "Te_variance_lst = np.zeros((len(ep_list), ))\n",
    "avg_Latency_lst = np.zeros((len(ep_list), ))\n",
    "avg_Inference_lst = np.zeros((len(ep_list), ))\n",
    "\n",
    "for idx in range(0, len(ep_list)):\n",
    "    Epochs = ep_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(ep_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Epochs', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(ep_list, [avg_Latency_lst, avg_Inference_lst], 'Epochs', labels = 'time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be15a7b-491d-4f50-91e3-6cb35376f32f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NeuralHD model fine-tuning\n",
    "Keep the batch size at **64**, varying the dimentionality, learning rate, regenloop, fractiontoDrop, and finally the epochs.\n",
    "\n",
    "    dim_list = [300, 400, 512, 1024, 2048, \n",
    "                4096, 8192, 16384, 32768, 65536] ->\n",
    "    \n",
    "    lr_list = [0.01, 0.02, 0.03, 0.04, 0.05,\n",
    "               0.001, 0.002, 0.003, 0.004, 0.005, \n",
    "               0.0001, 0.0002, 0.0003, 0.0004, 0.0005, \n",
    "               0.00001, 0.00002, 0.00003, 0.00004, 0.00005] -> \n",
    "               \n",
    "               \n",
    "    regenloop = [2, 3, 4, 5, 6, 7, 8, 9, 10] ->\n",
    "    \n",
    "    fractiontoDrop = [0.1, 0.2, 0.3, 0.4, 0.5,\n",
    "                      0.01, 0.02, 0.03, 0.04, 0.05] ->\n",
    "                      \n",
    "    ep_list = [4, 9, 16, 25, 36, \n",
    "               49, 64, 81, 100, 121, \n",
    "               144, 169, 196, 225, 256] -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e04c3-6b24-404b-b269-b14f4428f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup NeuralHD model's hyper-parameters\n",
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "# Dimension = 1024\n",
    "Learning_rate = 0.00001\n",
    "Regeneration = 25\n",
    "FractionDrop = 0.4\n",
    "Epochs = 16\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(dim_list), ))\n",
    "Tr_variance_lst = np.zeros((len(dim_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(dim_list), ))\n",
    "Te_variance_lst = np.zeros((len(dim_list), ))\n",
    "avg_Latency_lst = np.zeros((len(dim_list), ))\n",
    "avg_Inference_lst = np.zeros((len(dim_list), ))\n",
    "\n",
    "for idx in range(0, len(dim_list)):\n",
    "    Dimension = dim_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(dim_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Dimension', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(dim_list, [avg_Latency_lst, avg_Inference_lst], 'Dimension', labels = 'time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d3e32-61ef-4163-8b07-301ef4903e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "Dimension = 2048\n",
    "# Learning_rate = 0.0001\n",
    "Regeneration = 25\n",
    "FractionDrop = 0.4\n",
    "Epochs = 16\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(lr_list), ))\n",
    "Tr_variance_lst = np.zeros((len(lr_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(lr_list), ))\n",
    "Te_variance_lst = np.zeros((len(lr_list), ))\n",
    "avg_Latency_lst = np.zeros((len(lr_list), ))\n",
    "avg_Inference_lst = np.zeros((len(lr_list), ))\n",
    "\n",
    "for idx in range(0, len(lr_list)):\n",
    "    Learning_rate = lr_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(lr_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Learning rate', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(lr_list, [avg_Latency_lst, avg_Inference_lst], 'Learning rate', labels = 'time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ea319-9b48-4dcb-8c2a-93dd89ad5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "Dimension = 2048\n",
    "Learning_rate = 0.001\n",
    "# Regeneration = 4\n",
    "FractionDrop = 0.1\n",
    "Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(regenloop), ))\n",
    "Tr_variance_lst = np.zeros((len(regenloop), ))\n",
    "avg_Te_acc_lst = np.zeros((len(regenloop), ))\n",
    "Te_variance_lst = np.zeros((len(regenloop), ))\n",
    "avg_Latency_lst = np.zeros((len(regenloop), ))\n",
    "avg_Inference_lst = np.zeros((len(regenloop), ))\n",
    "\n",
    "for idx in range(0, len(regenloop)):\n",
    "    Regeneration = regenloop[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(regenloop, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Regeneration', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(regenloop, [avg_Latency_lst, avg_Inference_lst], 'Regeneration', labels = 'time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dbd26-4b5a-4362-8d2e-d30e90872d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "Dimension = 2048\n",
    "Learning_rate = 0.001\n",
    "Regeneration = 10\n",
    "# FractionDrop = 0.1\n",
    "Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(fractiontoDrop), ))\n",
    "Tr_variance_lst = np.zeros((len(fractiontoDrop), ))\n",
    "avg_Te_acc_lst = np.zeros((len(fractiontoDrop), ))\n",
    "Te_variance_lst = np.zeros((len(fractiontoDrop), ))\n",
    "avg_Latency_lst = np.zeros((len(fractiontoDrop), ))\n",
    "avg_Inference_lst = np.zeros((len(fractiontoDrop), ))\n",
    "\n",
    "for idx in range(0, len(fractiontoDrop)):\n",
    "    FractionDrop = fractiontoDrop[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(fractiontoDrop, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Fraction Drop', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(fractiontoDrop, [avg_Latency_lst, avg_Inference_lst], 'Fraction Drop', labels = 'time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e788293-7d74-499f-87cc-53199031d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "Dimension = 2048\n",
    "Learning_rate = 0.001\n",
    "Regeneration = 7\n",
    "FractionDrop = 0.4\n",
    "# Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(ep_list), ))\n",
    "Tr_variance_lst = np.zeros((len(ep_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(ep_list), ))\n",
    "Te_variance_lst = np.zeros((len(ep_list), ))\n",
    "avg_Latency_lst = np.zeros((len(ep_list), ))\n",
    "avg_Inference_lst = np.zeros((len(ep_list), ))\n",
    "\n",
    "for idx in range(0, len(ep_list)):\n",
    "    Epochs = ep_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(ep_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Epochs', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(ep_list, [avg_Latency_lst, avg_Inference_lst], 'Epochs', labels = 'time')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecac8bf-2541-44e5-afd1-b93017958aae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP model fine-tuning\n",
    "Keep the batch size at **64**, varying the learning rate and epochs.\n",
    "    \n",
    "    lr_list = [0.01, 0.02, 0.03, 0.04, 0.05,\n",
    "               0.001, 0.002, 0.003, 0.004, 0.005, \n",
    "               0.0001, 0.0002, 0.0003, 0.0004, 0.0005, \n",
    "               0.00001, 0.00002, 0.00003, 0.00004, 0.00005] -> 0.0004\n",
    "               \n",
    "    ep_list = [4, 9, 16, 25, 36, \n",
    "               49, 64, 81, 100, 121] -> 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a604d-8794-42d6-be36-f18887e046ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup MLP model's hyper-parameters\n",
    "The_model = 'MLP'\n",
    "Batch_size = 64\n",
    "# Learning_rate = 0.002\n",
    "Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(lr_list), ))\n",
    "Tr_variance_lst = np.zeros((len(lr_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(lr_list), ))\n",
    "Te_variance_lst = np.zeros((len(lr_list), ))\n",
    "avg_Latency_lst = np.zeros((len(lr_list), ))\n",
    "avg_Inference_lst = np.zeros((len(lr_list), ))\n",
    "\n",
    "for idx in range(0, len(lr_list)):\n",
    "    Learning_rate = lr_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(lr_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Learning rate', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(lr_list, [avg_Latency_lst, avg_Inference_lst], 'Learning rate', labels = 'time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42025493-adf2-4cee-901d-f960fd4e12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup MLP model's hyper-parameters\n",
    "batchSz_lst = [1,2,4,8,16,32,64,128,256,512]\n",
    "\n",
    "The_model = 'MLP'\n",
    "# Batch_size = 64\n",
    "Learning_rate = 0.0004\n",
    "Epochs = 36\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(batchSz_lst), ))\n",
    "Tr_variance_lst = np.zeros((len(batchSz_lst), ))\n",
    "avg_Te_acc_lst = np.zeros((len(batchSz_lst), ))\n",
    "Te_variance_lst = np.zeros((len(batchSz_lst), ))\n",
    "avg_Latency_lst = np.zeros((len(batchSz_lst), ))\n",
    "avg_Inference_lst = np.zeros((len(batchSz_lst), ))\n",
    "\n",
    "for idx in range(0, len(batchSz_lst)):\n",
    "    Batch_size = batchSz_lst[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(batchSz_lst, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Batch size', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(batchSz_lst, [avg_Latency_lst, avg_Inference_lst], 'Batch size', labels = 'time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7f8b6-5118-428c-a2a9-8e7d48a0ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup MLP model's hyper-parameters\n",
    "The_model = 'MLP'\n",
    "Batch_size = 64\n",
    "Learning_rate = 0.0004\n",
    "# Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(ep_list), ))\n",
    "Tr_variance_lst = np.zeros((len(ep_list), ))\n",
    "avg_Te_acc_lst = np.zeros((len(ep_list), ))\n",
    "Te_variance_lst = np.zeros((len(ep_list), ))\n",
    "avg_Latency_lst = np.zeros((len(ep_list), ))\n",
    "avg_Inference_lst = np.zeros((len(ep_list), ))\n",
    "\n",
    "for idx in range(0, len(ep_list)):\n",
    "    Epochs = ep_list[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(ep_list, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Epochs', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(ep_list, [avg_Latency_lst, avg_Inference_lst], 'Epochs', labels = 'time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108658b3-103d-403a-954a-d27c31ee335a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Support Vector Classifier(SVM.SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab57ab-ad54-42a4-b4ad-e5448eaf39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The_model = 'SVM'\n",
    "results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                          repeat_times = Repeat_times, model_select = The_model,\n",
    "                          verbose = False, Classes = Classes, Features = Features, \n",
    "                          Dimensions = 4, Learning_rate = Learning_rate, \n",
    "                          Epochs = Epochs, Batch_size = Batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06421e41-5aca-44ac-bfa5-5572070c857b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Increasing epochs but keeping dim = 1024, OnlineHD or NeuralHD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc286a08-d97d-4f47-b12b-7e97ca0ff887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_OHD = [5,10,50,100,200,300,400,500]\n",
    "ep_NHD = [1,2,10,20,40,60,80,100]\n",
    "print(f'Epochs: {ep_OHD} \\t {ep_NHD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becade17-ded3-4114-98aa-09ebe8bd2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "Dimension = 1024\n",
    "Learning_rate = 0.001\n",
    "Regeneration = 4\n",
    "FractionDrop = 0.4\n",
    "# Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(ep_NHD), ))\n",
    "Tr_variance_lst = np.zeros((len(ep_NHD), ))\n",
    "avg_Te_acc_lst = np.zeros((len(ep_NHD), ))\n",
    "Te_variance_lst = np.zeros((len(ep_NHD), ))\n",
    "avg_Latency_lst = np.zeros((len(ep_NHD), ))\n",
    "avg_Inference_lst = np.zeros((len(ep_NHD), ))\n",
    "\n",
    "for idx in range(0, len(ep_NHD)):\n",
    "    Epochs = ep_NHD[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(ep_NHD, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Epochs', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(ep_NHD, [avg_Latency_lst, avg_Inference_lst], 'Epochs', labels = 'time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc85908-d0c5-4c19-9cc4-89108aa2fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup OnlineHD model's hyper-parameters\n",
    "The_model = 'OnlineHD'\n",
    "Batch_size = 64\n",
    "Dimensions= 1024 \n",
    "Learning_rate = 1e-05\n",
    "# Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(ep_OHD), ))\n",
    "Tr_variance_lst = np.zeros((len(ep_OHD), ))\n",
    "avg_Te_acc_lst = np.zeros((len(ep_OHD), ))\n",
    "Te_variance_lst = np.zeros((len(ep_OHD), ))\n",
    "avg_Latency_lst = np.zeros((len(ep_OHD), ))\n",
    "avg_Inference_lst = np.zeros((len(ep_OHD), ))\n",
    "\n",
    "for idx in range(0, len(ep_OHD)):\n",
    "    Epochs = ep_OHD[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(ep_OHD, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Epochs', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(ep_OHD, [avg_Latency_lst, avg_Inference_lst], 'Epochs', labels = 'time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e985b-1c20-46f0-9eb3-75c52aaa7a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cb0ec-56a4-4782-b67c-9fe4a039b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_models = ['SVM', 'MLP', 'OnlineHD', 'NeuralHD']\n",
    "accuracies = [89.3265, 96.0195, 95.0221, 95.2724]\n",
    "training_latencies = [1.2818, 19.8972, 13.1704, 49.7349]\n",
    "inference_latencies = [3.3513, 0.1986, 0.2481, 0.1009]\n",
    "\n",
    "plt.bar(All_models, accuracies)\n",
    "plt.title('Accuracy by models')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(All_models, training_latencies)\n",
    "plt.title('Training Time Latency by models')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(All_models, inference_latencies)\n",
    "plt.title('Inference Time by models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130072a-27d6-4dc7-9861-602cdf55050d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More experiments and tasks on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc9f22-ca5f-46b4-9851-8ce07d847f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "regenloops = [2,5,10,13,15,20,25,31]\n",
    "\n",
    "The_model = 'NeuralHD'\n",
    "Batch_size = 64\n",
    "Dimension = 2048\n",
    "Learning_rate = 0.001\n",
    "# Regeneration = 4\n",
    "FractionDrop = 0.4\n",
    "Epochs = 32\n",
    "\n",
    "# Y_axis\n",
    "avg_Tr_acc_lst = np.zeros((len(regenloops), ))\n",
    "Tr_variance_lst = np.zeros((len(regenloops), ))\n",
    "avg_Te_acc_lst = np.zeros((len(regenloops), ))\n",
    "Te_variance_lst = np.zeros((len(regenloops), ))\n",
    "avg_Latency_lst = np.zeros((len(regenloops), ))\n",
    "avg_Inference_lst = np.zeros((len(regenloops), ))\n",
    "\n",
    "for idx in range(0, len(regenloops)):\n",
    "    Regeneration = regenloops[idx]\n",
    "    results = Models_Analyzer(x_train, x_test, y_train, y_test, \n",
    "                              repeat_times = Repeat_times, model_select = The_model,\n",
    "                              verbose = False, Classes = Classes, Features = Features, \n",
    "                              Dimensions = Dimension, Learning_rate = Learning_rate, \n",
    "                              Epochs = Epochs, Batch_size = Batch_size,\n",
    "                              Regeneration = Regeneration, \n",
    "                              FractionDrop = FractionDrop)\n",
    "    \n",
    "    avg_Tr_acc_lst[idx] = results[0]\n",
    "    Tr_variance_lst[idx] = results[1]\n",
    "    avg_Te_acc_lst[idx] = results[2]\n",
    "    Te_variance_lst[idx] = results[3]\n",
    "    avg_Latency_lst[idx] = results[4]\n",
    "    avg_Inference_lst[idx] = results[5]\n",
    "\n",
    "\n",
    "print(Evaluation_Visualizer(regenloops, [avg_Tr_acc_lst, avg_Te_acc_lst, Tr_variance_lst, Te_variance_lst], 'Regeneration', labels = 'acc'))\n",
    "print(Evaluation_Visualizer(regenloops, [avg_Latency_lst, avg_Inference_lst], 'Regeneration', labels = 'time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd126015-cb47-48eb-b1d1-51ddf4cf2e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
